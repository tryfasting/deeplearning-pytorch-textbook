{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN의 입출력 단위가 단어 레벨(word-level)이 아니라 문자 레벨(character-level)로 하여 RNN을 구현한다면, 이를 문자 단위 RNN이라고 한다. 다대다 구조로 구현해보자. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 문자 단위 RNN(Char RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 훈련 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력 데이터와 레이블 데이터에 대해서 문자 집합(vocabulary)를 만든다.  \n",
    "여기서 문자 집합은 중복을 제거한 문자들의 집합이다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 집합의 크기 : 5\n"
     ]
    }
   ],
   "source": [
    "input_str = 'apple'\n",
    "label_str = 'pple!'\n",
    "char_vocab = sorted(list(set(input_str+label_str)))\n",
    "vocab_size = len(char_vocab)\n",
    "print(f'문자 집합의 크기 : {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문자 집합의 문자는 총 5개이다. '!,a,e,l,p'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 하이퍼파라미터를 정의한다.  \n",
    "입력은 원-핫 벡터를 사용할 것이므로 입력의 크기는 문자 집합의 크기여야만 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = vocab_size # 입력의 크기는 문자 집합의 크기\n",
    "hidden_size = 5\n",
    "output_size = 5\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문자 집합에 고유한 정수를 부여한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n"
     ]
    }
   ],
   "source": [
    "char_to_index = dict((c,i) for i,c in enumerate(char_vocab))\n",
    "# 문자에 고유한 정수 인덱스 부여\n",
    "print(char_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "나중에 예측 결과를 다시 문자 시퀀스로 보기 위해서   \n",
    "반대로 정수로부터 문자를 얻을 수 있는 index_to_char를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
     ]
    }
   ],
   "source": [
    "index_to_char = {}\n",
    "for key, value in char_to_index.items():\n",
    "    index_to_char[value] = key\n",
    "\n",
    "print(index_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 입력 데이터와 레이블 데이터의 각 문자들을 정수로 맵핑한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 4, 3, 2]\n",
      "[4, 4, 3, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "x_data = [char_to_index[c] for c in input_str]\n",
    "y_data = [char_to_index[c] for c in label_str]\n",
    "print(x_data)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이토치의 nn.RNN()은 기본적으로 3차원 텐서를 입력받는다.  \n",
    "따라서 배치 차원을 추가해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4, 4, 3, 2]]\n",
      "[[4, 4, 3, 2, 0]]\n"
     ]
    }
   ],
   "source": [
    "# 배치 차원 추가\n",
    "# 텐서 연산인 unsqueeze(0)를 통해 해결할 수도 있음.\n",
    "x_data = [x_data]\n",
    "y_data = [y_data]\n",
    "print(x_data)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력 시퀀스의 각 문자들을 원-핫 벡터로 바꿔준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0.]])]\n"
     ]
    }
   ],
   "source": [
    "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
    "print(x_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력 데이터와 레이블 데이터를 텐서로 바꿔준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_17804\\2348034151.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  X = torch.FloatTensor(x_one_hot)\n"
     ]
    }
   ],
   "source": [
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 각 텐서의 크기를 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 크기 : torch.Size([1, 5, 5])\n",
      "레이블의 크기 : torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "print(f'훈련 데이터의 크기 : {X.shape}')\n",
    "print(f'레이블의 크기 : {Y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 모델 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 RNN 모델을 구현하자.  \n",
    "fc는 fully-connected layer를 뜻하며, 출력층으로 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size,hidden_size,batch_first=True) # RNN 셀 구현\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
    "\n",
    "    def forward(self, x): # 구현한 RNN 셀과 출력층을 연결\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "클래스로 정의한 모델을 net에 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 입력된 모델에 입력을 넣어서 출력의 크기를 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "outputs = net(X)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1,5,5)의 크기를 갖는데, 각각  \n",
    "배치 차원,  \n",
    "시점,  \n",
    "출력의 크기   \n",
    "이다. 나중에 정확도를 측정할 때는 이를 모두 펼쳐서 계산하게 되는데,  \n",
    "이때는 view를 사용하여 배치 차원과 시점 차원을 하나로 만든다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.view(-1, input_size).shape) # 2차원 텐서로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 레이블 데이터의 크기를 리마인드 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "print(Y.shape)\n",
    "print(Y.view(-1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "레이블 데이터는 (1,5)의 크기를 가지는데,   \n",
    "마찬가지로 나중에 정확도를 측정할 때는 이를 펼쳐서 계산할 예정이다.   \n",
    "이 경우 (5)의 크기를 가지게 된다.  \n",
    "\n",
    "이제 옵티마이저와 손실 함수를 정의한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(),learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss : 1.6507171392440796\n",
      "prediction : [[2 2 2 2 2]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : eeeee\n",
      "1 loss : 1.3870229721069336\n",
      "prediction : [[2 4 4 2 2]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : eppee\n",
      "2 loss : 1.1303677558898926\n",
      "prediction : [[4 4 4 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pppe!\n",
      "3 loss : 0.8727928400039673\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "4 loss : 0.6350645422935486\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "5 loss : 0.4534829258918762\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "6 loss : 0.3227476179599762\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "7 loss : 0.22444100677967072\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "8 loss : 0.1529284566640854\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "9 loss : 0.10256731510162354\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "10 loss : 0.06821970641613007\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "11 loss : 0.04539981484413147\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "12 loss : 0.030594807118177414\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "13 loss : 0.021108191460371017\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "14 loss : 0.014986375346779823\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "15 loss : 0.010952593758702278\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "16 loss : 0.008225558325648308\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "17 loss : 0.006334180943667889\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "18 loss : 0.0049906084313988686\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "19 loss : 0.004014910664409399\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "20 loss : 0.0032917973585426807\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "21 loss : 0.002745671896263957\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "22 loss : 0.0023262009490281343\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "23 loss : 0.0019988003186881542\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "24 loss : 0.0017395347822457552\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "25 loss : 0.0015316089848056436\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "26 loss : 0.0013628449523821473\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "27 loss : 0.0012243837118148804\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "28 loss : 0.0011095947120338678\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "29 loss : 0.001013652654364705\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "30 loss : 0.0009328238666057587\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "31 loss : 0.0008642068132758141\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "32 loss : 0.0008055651560425758\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "33 loss : 0.0007550664595328271\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "34 loss : 0.0007114256732165813\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "35 loss : 0.0006734764901921153\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "36 loss : 0.0006403381703421474\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "37 loss : 0.0006112488335929811\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "38 loss : 0.0005855893832631409\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "39 loss : 0.0005629074294120073\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "40 loss : 0.0005427743308246136\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "41 loss : 0.0005248567322269082\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "42 loss : 0.0005087972385808825\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "43 loss : 0.0004944053944200277\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "44 loss : 0.00048153838724829257\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "45 loss : 0.0004698864067904651\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "46 loss : 0.000459330331068486\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "47 loss : 0.00044979891390539706\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "48 loss : 0.00044117277138866484\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "49 loss : 0.00043321383418515325\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "50 loss : 0.0004259934648871422\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "51 loss : 0.00041934504406526685\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "52 loss : 0.000413292262237519\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "53 loss : 0.00040769221959635615\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "54 loss : 0.0004025210510008037\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "55 loss : 0.00039768352871760726\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "56 loss : 0.0003932987747248262\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "57 loss : 0.00038908078568056226\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "58 loss : 0.0003852678928524256\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "59 loss : 0.00038169330218806863\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "60 loss : 0.00037821399746462703\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "61 loss : 0.0003750444739125669\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "62 loss : 0.00037204177351668477\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "63 loss : 0.00036915819509886205\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "64 loss : 0.0003664414689410478\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "65 loss : 0.0003638438356574625\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "66 loss : 0.0003614130546338856\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "67 loss : 0.00035907753044739366\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "68 loss : 0.00035678973654285073\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "69 loss : 0.00035459722857922316\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "70 loss : 0.00035252387169748545\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "71 loss : 0.0003505219938233495\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "72 loss : 0.00034856778802350163\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "73 loss : 0.0003466612542979419\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "74 loss : 0.00034480233443900943\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "75 loss : 0.00034301492269150913\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "76 loss : 0.00034122756915166974\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "77 loss : 0.00033953547244891524\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "78 loss : 0.0003378910187166184\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "79 loss : 0.00033629429526627064\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "80 loss : 0.00033462600549682975\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "81 loss : 0.0003330768959131092\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "82 loss : 0.000331503979396075\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "83 loss : 0.000329954840708524\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "84 loss : 0.00032854871824383736\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "85 loss : 0.00032702344469726086\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "86 loss : 0.0003255458432249725\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "87 loss : 0.00032409201958216727\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "88 loss : 0.0003227097331546247\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "89 loss : 0.0003213274176232517\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "90 loss : 0.000319897459121421\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "91 loss : 0.0003185151726938784\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "92 loss : 0.00031715669319964945\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "93 loss : 0.0003157982137054205\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "94 loss : 0.0003144873771816492\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "95 loss : 0.00031315276282839477\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "96 loss : 0.00031184195540845394\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "97 loss : 0.0003104834468103945\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "98 loss : 0.00030924411839805543\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n",
      "99 loss : 0.0003079333109781146\n",
      "prediction : [[4 4 3 2 0]]\n",
      "true Y : [[4, 4, 3, 2, 0]]\n",
      " prediction str : pple!\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X)\n",
    "    # view를 하는 이유는 Batch 차원 제거를 위함.\n",
    "    loss = criterion(outputs.view(-1, input_size), Y.view(-1))\n",
    "    loss.backward() # 기울기 계산\n",
    "    optimizer.step() # 아까 optimizer 선언 시 넣어둔 파라미터 업데이트\n",
    "\n",
    "    # 아래 세 줄은 모델이 실제 어떻게 예측했는지를 확인하는 코드.\n",
    "    result = outputs.data.numpy().argmax(axis=2) # 최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\n",
    "\n",
    "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
    "\n",
    "    print(i, f'loss : {loss.item()}\\nprediction : {result}\\ntrue Y : {y_data}\\n prediction str : {result_str}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 더 많은 데이터로 학습한 문자 단위 RNN(Char RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 훈련 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "            \"collect wood and don't assign them tasks and work, but rather \"\n",
    "            \"teach them to long for the endless immensity of the sea.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문자 집합을 생성하고, 각 문자에 고유한 정수를 부여한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_set = list(set(sentence)) # 중복을 제거한 문자 집합 생성\n",
    "char_dic = {c : i for i, c in enumerate(char_set)} # 각 문자에 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'g': 0, 's': 1, 'l': 2, 'y': 3, 'r': 4, 'i': 5, '.': 6, 'u': 7, 't': 8, 'e': 9, 'f': 10, 'a': 11, 'p': 12, ' ': 13, 'c': 14, \"'\": 15, 'b': 16, 'w': 17, 'd': 18, 'm': 19, ',': 20, 'h': 21, 'k': 22, 'o': 23, 'n': 24}\n"
     ]
    }
   ],
   "source": [
    "print(char_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 문자에 정수가 부여되었으며, 총 25개의 문자가 존재한다.  \n",
    "문자 집합의 크기를 확인해보자.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 집합의 크기 : 25\n"
     ]
    }
   ],
   "source": [
    "dic_size = len(char_dic)\n",
    "print(f'문자 집합의 크기 : {dic_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문자 집합의 크기는 25이며,  \n",
    "입력은 원-핫 벡터로 사용할 것이므로 매 시점마다 들어갈 입력의 크기이기도 하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 하이퍼파라미터를 설정하자.  \n",
    "hidden_size(은닉 상태의 크기)를 입력의 크기와 동일하게 줬는데,  \n",
    "이는 사용자의 선택으로 다른 값을 줘도 무방하다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 sequence_length라는 변수를 선언했는데,  \n",
    "우리가 앞서 만든 샘플을 10개 단위로 끊어서 샘플을 만들 예정이기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "hidden_size = dic_size\n",
    "sequence_length = 10   # 임의 숫자 지정\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 임의로 지정한 sequence_length 값인 10의 단위로 샘플들을 잘라서 데이터를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 if you wan -> f you want\n",
      "1 f you want ->  you want \n",
      "2  you want  -> you want t\n",
      "3 you want t -> ou want to\n",
      "4 ou want to -> u want to \n",
      "5 u want to  ->  want to b\n",
      "6  want to b -> want to bu\n",
      "7 want to bu -> ant to bui\n",
      "8 ant to bui -> nt to buil\n",
      "9 nt to buil -> t to build\n",
      "10 t to build ->  to build \n",
      "11  to build  -> to build a\n",
      "12 to build a -> o build a \n",
      "13 o build a  ->  build a s\n",
      "14  build a s -> build a sh\n",
      "15 build a sh -> uild a shi\n",
      "16 uild a shi -> ild a ship\n",
      "17 ild a ship -> ld a ship,\n",
      "18 ld a ship, -> d a ship, \n",
      "19 d a ship,  ->  a ship, d\n",
      "20  a ship, d -> a ship, do\n",
      "21 a ship, do ->  ship, don\n",
      "22  ship, don -> ship, don'\n",
      "23 ship, don' -> hip, don't\n",
      "24 hip, don't -> ip, don't \n",
      "25 ip, don't  -> p, don't d\n",
      "26 p, don't d -> , don't dr\n",
      "27 , don't dr ->  don't dru\n",
      "28  don't dru -> don't drum\n",
      "29 don't drum -> on't drum \n",
      "30 on't drum  -> n't drum u\n",
      "31 n't drum u -> 't drum up\n",
      "32 't drum up -> t drum up \n",
      "33 t drum up  ->  drum up p\n",
      "34  drum up p -> drum up pe\n",
      "35 drum up pe -> rum up peo\n",
      "36 rum up peo -> um up peop\n",
      "37 um up peop -> m up peopl\n",
      "38 m up peopl ->  up people\n",
      "39  up people -> up people \n",
      "40 up people  -> p people t\n",
      "41 p people t ->  people to\n",
      "42  people to -> people tog\n",
      "43 people tog -> eople toge\n",
      "44 eople toge -> ople toget\n",
      "45 ople toget -> ple togeth\n",
      "46 ple togeth -> le togethe\n",
      "47 le togethe -> e together\n",
      "48 e together ->  together \n",
      "49  together  -> together t\n",
      "50 together t -> ogether to\n",
      "51 ogether to -> gether to \n",
      "52 gether to  -> ether to c\n",
      "53 ether to c -> ther to co\n",
      "54 ther to co -> her to col\n",
      "55 her to col -> er to coll\n",
      "56 er to coll -> r to colle\n",
      "57 r to colle ->  to collec\n",
      "58  to collec -> to collect\n",
      "59 to collect -> o collect \n",
      "60 o collect  ->  collect w\n",
      "61  collect w -> collect wo\n",
      "62 collect wo -> ollect woo\n",
      "63 ollect woo -> llect wood\n",
      "64 llect wood -> lect wood \n",
      "65 lect wood  -> ect wood a\n",
      "66 ect wood a -> ct wood an\n",
      "67 ct wood an -> t wood and\n",
      "68 t wood and ->  wood and \n",
      "69  wood and  -> wood and d\n",
      "70 wood and d -> ood and do\n",
      "71 ood and do -> od and don\n",
      "72 od and don -> d and don'\n",
      "73 d and don' ->  and don't\n",
      "74  and don't -> and don't \n",
      "75 and don't  -> nd don't a\n",
      "76 nd don't a -> d don't as\n",
      "77 d don't as ->  don't ass\n",
      "78  don't ass -> don't assi\n",
      "79 don't assi -> on't assig\n",
      "80 on't assig -> n't assign\n",
      "81 n't assign -> 't assign \n",
      "82 't assign  -> t assign t\n",
      "83 t assign t ->  assign th\n",
      "84  assign th -> assign the\n",
      "85 assign the -> ssign them\n",
      "86 ssign them -> sign them \n",
      "87 sign them  -> ign them t\n",
      "88 ign them t -> gn them ta\n",
      "89 gn them ta -> n them tas\n",
      "90 n them tas ->  them task\n",
      "91  them task -> them tasks\n",
      "92 them tasks -> hem tasks \n",
      "93 hem tasks  -> em tasks a\n",
      "94 em tasks a -> m tasks an\n",
      "95 m tasks an ->  tasks and\n",
      "96  tasks and -> tasks and \n",
      "97 tasks and  -> asks and w\n",
      "98 asks and w -> sks and wo\n",
      "99 sks and wo -> ks and wor\n",
      "100 ks and wor -> s and work\n",
      "101 s and work ->  and work,\n",
      "102  and work, -> and work, \n",
      "103 and work,  -> nd work, b\n",
      "104 nd work, b -> d work, bu\n",
      "105 d work, bu ->  work, but\n",
      "106  work, but -> work, but \n",
      "107 work, but  -> ork, but r\n",
      "108 ork, but r -> rk, but ra\n",
      "109 rk, but ra -> k, but rat\n",
      "110 k, but rat -> , but rath\n",
      "111 , but rath ->  but rathe\n",
      "112  but rathe -> but rather\n",
      "113 but rather -> ut rather \n",
      "114 ut rather  -> t rather t\n",
      "115 t rather t ->  rather te\n",
      "116  rather te -> rather tea\n",
      "117 rather tea -> ather teac\n",
      "118 ather teac -> ther teach\n",
      "119 ther teach -> her teach \n",
      "120 her teach  -> er teach t\n",
      "121 er teach t -> r teach th\n",
      "122 r teach th ->  teach the\n",
      "123  teach the -> teach them\n",
      "124 teach them -> each them \n",
      "125 each them  -> ach them t\n",
      "126 ach them t -> ch them to\n",
      "127 ch them to -> h them to \n",
      "128 h them to  ->  them to l\n",
      "129  them to l -> them to lo\n",
      "130 them to lo -> hem to lon\n",
      "131 hem to lon -> em to long\n",
      "132 em to long -> m to long \n",
      "133 m to long  ->  to long f\n",
      "134  to long f -> to long fo\n",
      "135 to long fo -> o long for\n",
      "136 o long for ->  long for \n",
      "137  long for  -> long for t\n",
      "138 long for t -> ong for th\n",
      "139 ong for th -> ng for the\n",
      "140 ng for the -> g for the \n",
      "141 g for the  ->  for the e\n",
      "142  for the e -> for the en\n",
      "143 for the en -> or the end\n",
      "144 or the end -> r the endl\n",
      "145 r the endl ->  the endle\n",
      "146  the endle -> the endles\n",
      "147 the endles -> he endless\n",
      "148 he endless -> e endless \n",
      "149 e endless  ->  endless i\n",
      "150  endless i -> endless im\n",
      "151 endless im -> ndless imm\n",
      "152 ndless imm -> dless imme\n",
      "153 dless imme -> less immen\n",
      "154 less immen -> ess immens\n",
      "155 ess immens -> ss immensi\n",
      "156 ss immensi -> s immensit\n",
      "157 s immensit ->  immensity\n",
      "158  immensity -> immensity \n",
      "159 immensity  -> mmensity o\n",
      "160 mmensity o -> mensity of\n",
      "161 mensity of -> ensity of \n",
      "162 ensity of  -> nsity of t\n",
      "163 nsity of t -> sity of th\n",
      "164 sity of th -> ity of the\n",
      "165 ity of the -> ty of the \n",
      "166 ty of the  -> y of the s\n",
      "167 y of the s ->  of the se\n",
      "168  of the se -> of the sea\n",
      "169 of the sea -> f the sea.\n"
     ]
    }
   ],
   "source": [
    "# 데이터 구성\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    # 슬라이싱 길이 계산 : (끝 인덱스 - 시작 인덱스)가 슬라이싱 길이이다. \n",
    "    x_str = sentence[i:i + sequence_length]\n",
    "    y_str = sentence[i + 1: i + sequence_length + 1]\n",
    "    print(i, x_str, '->', y_str)\n",
    "\n",
    "    x_data.append([char_dic[c] for c in x_str])\n",
    "    y_data.append([char_dic[c] for c in y_str])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### range 횟수(슬라이싱 가능 횟수) 설정이 어떻게 저렇게 나왔을까?\n",
    "- x_str,y_str에 마지막 인덱스(len(sentence) - sequence_length - 1)가 넘겨졌을 때를 보자.  \n",
    "- x_str       \n",
    "= (len(sentence) - sequence_length - 1) : (len(sentence) - sequence_length -1 + sequence_length)  \n",
    "= len(sentence) - sequence_length - 1 : len(sentence) - 1\n",
    "\n",
    "- y_str  \n",
    "= (len(sentence) - sequence_length - 1 + 1): (len(sentence) - sequence_length - 1 + sequence_length + 1)  \n",
    "= len(sentence) - sequence_length : len(sentence)\n",
    "\n",
    "핵심은 y_str의 슬라이싱 범위의 마지막에 있다.  \n",
    "(i + sequence_length + 1)이면,  \n",
    "range의 마지막 인덱스(len(sentence) - sequence_length - 1)가 넘겨지면, len(sentence)가 된다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "총 170개의 샘플이 생성되었다.  \n",
    "그리고 각 샘플의 각 문자들은 고유한 정수로 인코딩이 된 상태이다.  \n",
    "첫 번째 샘플의 입력 데이터와 레이블 데이터를 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 10, 13, 3, 23, 7, 13, 17, 11, 24]\n",
      "[10, 13, 3, 23, 7, 13, 17, 11, 24, 8]\n"
     ]
    }
   ],
   "source": [
    "print(x_data[0])\n",
    "print(y_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력 시퀀스에 대해 원-핫 인코딩을 수행하고, 입력 데이터와 레이블 데이터를 텐서로 변환한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.eye()는 주어진 크기의 단위행렬(혹은 대각행렬) 생성\n",
    "# 단위행렬은, 주대각선(왼쪽 상단에서 오른쪽 하단 방향) 원소가 1이고, 나머지는 모두 0인 행렬\n",
    "# dic_size는 문자 집합의 크기\n",
    "\n",
    "# x_data는 단위문자를 정수로 맵핑한 데이터였다.\n",
    "# np.eye(dic_size)에 [x]를 붙이면, 단위행렬의 x번째 행을 선택하게 된다.\n",
    "# 예컨대 np.eye(3)[0] -> [1.,0.,0.]이 되는것이다.\n",
    "# 즉 맵핑된 정수의 인덱스에 '1'을 넣는 것과 같은 효과가 나온다.\n",
    "# 따라서 원-핫 인코딩이 된다.\n",
    "x_one_hot = [np.eye(dic_size)[x] for x in x_data] # x 데이터는 원-핫 인코딩\n",
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 크기 : torch.Size([170, 10, 25])\n",
      "레이블의 크기 : torch.Size([170, 10])\n"
     ]
    }
   ],
   "source": [
    "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
    "print('레이블의 크기 : {}'.format(Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(X[0])\n",
    "'''tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "         0., 0., 0., 0., 0., 0., 0.], i\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
    "         0., 0., 0., 0., 0., 0., 0.], f\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
    "         0., 0., 0., 0., 0., 0., 0.], 공백\n",
    "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "         0., 0., 0., 0., 0., 0., 0.], y\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "         0., 0., 0., 0., 0., 1., 0.], o\n",
    "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "         0., 0., 0., 0., 0., 0., 0.], y\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
    "         0., 0., 0., 0., 0., 0., 0.], 공백\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
    "         0., 0., 0., 0., 0., 0., 0.], w\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
    "         0., 0., 0., 0., 0., 0., 0.], a\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "         0., 0., 0., 0., 0., 0., 1.]]) n '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 13,  3, 23,  7, 13, 17, 11, 24,  8])\n"
     ]
    }
   ],
   "source": [
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 모델 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 실습한 문자 단위 RNN과 거의 동일하다.  \n",
    "단 이번에는 은닉층을 두 개 쌓는다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layers):\n",
    "        # 현재 hidden_size는 dic_size와 같음.\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(dic_size, hidden_size, 2) # 이번에는 층이 2개이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.RNN() 안에 num_layers라는 인자를 사용한다. 은닉층을 몇 개 쌓을 것인지를 설정한다.  \n",
    "모델 선언 시 layers라는 인자에 2를 전달하여 은닉층을 두 개 쌓는다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 비용함수와 옵티마이저를 선언한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 모델에 입력을 넣어서 출력의 크기를 확인해본다.  \n",
    "각각, (배치 차원, 시점, 출력의 크기)이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([170, 10, 25])\n"
     ]
    }
   ],
   "source": [
    "outputs = net(X)\n",
    "print(outputs.shape) # 3차원 텐서"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "나중에 정확도를 측정할 때는 이를 모두 펼쳐서 계산한다.  \n",
    "view를 사용하여 배치 차원과 시점 차원을 하나로 만든다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1700, 25])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.view(-1,dic_size).shape) # 2차원 텐서로 전환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "잠깐 레이블 데이터의 크기를 리마인드해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([170, 10])\n",
      "torch.Size([1700])\n"
     ]
    }
   ],
   "source": [
    "print(Y.shape)\n",
    "print(Y.view(-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ha    a           a     a a'             a               a                   aa          a           a   aa               a      a    h           a      aa        aa              \n",
      "                                                                                                                                                                                   \n",
      "t''.dt'',dhthddhdht dhdhdlddthdthed'dhhthddhddhddhhdlddt'dthdkhdmddht.edtt,edmp dttedbdht'dhhedhdhddhtt,dtt dttedhdlt'dht'dhht,hephddhdhddddtt.ddht ddhedlddhtt,dhthdtddthdphddhedc\n",
      "t lc  ia     e e      e            o    o           e        e       e  e        e            o              l       o        e       l          e       o                        o\n",
      " et tr tr  th st   t  e it  t  tett ehst  ktet tt  t       heh ih thh ht    sttse sh  she   tt ei st st eh  s th  ehe s e ittst it t h  st st ehh t e sh  thst h  s  et   o  i  tt \n",
      " tt t so tttototo totototodototosototodt no tooo tototheoto totodotoo otost totototototodot otoht totototototototototototott totodotost totototototoeotododotototott oklt t  t sodo\n",
      " dtht h at tow htls tot e ahthtoh aow ht w htrhtot als er'tht h r tt tos htot t t u ao  at aoerhtot t a   tow t tot w therht trh dhtrht tohtr t t t elt a doh a e  t tostotehtrs a \n",
      " r o  t np t a  tlrht t tlr t rrt tow  tp    n tnt il  erht rr  nr   rpt  tot r rlt ar  ilp  er   rpt tls t rlt tot r t tl t rrt eo r   rr  t t r t ar  rlrp  ilr r   o th thtrt ir\n",
      " n  r t nt epr  alr tnt els t r t tpt trp  t t ntt gct  r t r t rpt  rpr  tnt t rpt tp  gct  ir   r t tps t rpt t t i t er t r t notn t r t ntt r t ers rlrp  gp  nt g  t  t  nt rn\n",
      "lnt g tnss go  td n ttt eps t dpt e    ts tt p nss g t er t g t r  t go n tnd todpt es  gntt gn tog t i t dtn t tot epther t gct n en tog  ns d g t gcs d dp  gp  ns g  es t ens gs\n",
      "lnto  tmstheo  tdln tothem  tod't to m to tt o nsshg them thg o l  t go n tod todkt ts i ntthem to ot tos ton t tot eoshem thgch nhem to  on  t g thems d nns em  ns e  ms thems gc\n",
      "loton toso eo lotln tot dln todlt aoum tolto oldnth lth m thdlollltt emlc tos t d t ao  andthem to kt tos ton d tot dodhem thnlh dhem to lon  tod thdlsad dcs eln n  io mo them  nc\n",
      "loton'tonthto cudld tothem  aon't toum to tocoldnto lthem to lo lett emld tos ton't tosiandthem to ks tos ton d tot todhem to ch ahem to conc ton themsodldositlnens eo tn th m  ec\n",
      "loton'tonthto cudld tot em  aon't aoem to tosd eoth  ther to co leet aold tos ton't assiunethem to kt aos ton t tut todhem to ch ahem to cunc ton thecsod uetitlnensieo tn thems ec\n",
      "lobon'tons to euild andher  aon't aoem to tu d e to  ther to co leet ao d tod ton't ansilnethem to kt ans ton t tut todher to 'h ahem to conl ton thersod u siimnens io pn themsins\n",
      "lowon'tons to build andhero aon't aoem tfett s esthe ther to bo lens tond tnd won't ansiinether totks wnd wo  t tut wodher toech ther to cunc to, thersodlensiinnens io p, themsiws\n",
      "lobon tans to build andher, aon t doum tfepenole to  ther to lo le t wo ' tnd ton t dnsiinethem totki tnd wor d tut audher to ch them to lonc to  thersndlensiinners wy p, thems ns\n",
      "lobon tass to build anshir, aon't doum tpepfnole tog ther to lollect wo d tnd ton't dssign them tosss tnd tor d tut authem togch them to lond ton themsnd essiimnensiiy tf thems as\n",
      "loton tast to build anshem, aon't doum tpedenole tog ther to collect wo d tnd aon't dssign them tosks tnd aogk, tut a them togch them to lond tor themsnd essiimeensiiy tf themsias\n",
      "loton tant to build anshem, don't doum up penole togcther to collect wood and aon't dssign them tosks tnd aork, tut dothem to ch them to lond ton toertnd ecsiimeensiiy tf themshas\n",
      "m,ton want to build dnshem, don't doum up penole thg ther to collect wood and aon't dssign them tosks tnd aork, tut aothem torch them to lonc torktoersndlecsiimnensiih pf themshac\n",
      "m,tonlwant to build dnshem, don't doum up penple thg ther to collect wood and don't dssign them tosks dnd aork, tut dothem toach them to lond torkthersndlecs im ensifh pf themsias\n",
      "a,tonlwant to build anshir, don't doum up people tog ther to collect wood and don't dssign them tosks tnd aodk, tut aother toach them to long tor toersndlecs im ensify mf themshas\n",
      "r,ton went to build anshir, don't doum up people tog ther to collect wo d tnd don't dssign them tosks and work, tut aather toach them to long tor thersndlecsiim ensify of themshas\n",
      "p ton want to build anshir, don't aoum up people tog ther to collect wo d ans don't assign them tosks and work, but aather toach them to long tor therendlessiim ensify of themshas\n",
      "p ton want to build anshir, don't aoum up people together to collect wo d and don't assign them tasks and work, but aather toach them to long for themendlessiimmensiiy of themehas\n",
      "p bon want to build anship, don't arum up people thgether to collect wo d and dor't assign them tasks and work, but aather toach them to long for themendlessiimmensiiy of themehas\n",
      "p won want to build anship, don't arum up people together to collect wo d and dor't assign them tasks and work, but gather toach them to long for the sndlessiimmensity of thershas\n",
      "p won tant to build a ship, don't arum up people to ether to collect wo d and don't assign them tosks and dork, but rather toach them to long for the endless immensity of the ehas\n",
      "p won want to build a ship, don't arum up people together to collect wood and don't assign them tosks and dork, but rather toach them to long for the endless immensity of the shgd\n",
      "l wou want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and dork, but rather toach them to long for the endless immensity of the shad\n",
      "p wou want to build a ship, don't drum up people to ether to collect wood and don't dssign them tasks and dork, but rather toach them ta long for toe endless immensity of the seas\n",
      "p dou want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to cong for the sndless immensity of therseas\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the eeas\n",
      "p yon want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the seas\n",
      "p yon want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the eeas\n",
      "p yon want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the eeas\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the sead\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the sead\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seas\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather toach them to cong for the endless immensity of the seac\n",
      "l you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for the endless immensity of the seac\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and dork, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X) # (170, 10, 25) 크기를 가진 텐서를 매 에포트마다 모델의 입력으로 사용\n",
    "\n",
    "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # results의 텐서 크기는 (170,10)\n",
    "    results = outputs.argmax(dim=2)\n",
    "    predict_str = ''\n",
    "    for j, result in enumerate(results):\n",
    "        if j == 0: # 처음에는 예측 결과를 전부 가져오지만\n",
    "            predict_str += ''.join([char_set[t] for t in result])\n",
    "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
    "            predict_str += char_set[result[-1]]\n",
    "\n",
    "    print(predict_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
