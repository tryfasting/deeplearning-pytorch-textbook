{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "깊은 인공 신경망을 학습하다보면 역전파 과정에서 입력층으로 갈수록 기울기(Gradient)가 점차적으로 작아지는 현상이 발생할 수 있다. 입력층에 가까운층들에서 가중치들이 업데이트가 제대로 되지 않으면 결국 최적의 모델을 찾을 수 없게 된다. 이를 기울기 소실이라고 한다.  \n",
    "\n",
    "반대의 경우도 있다. 기울기가 점차 커지더니 가중치들이 비정상적으로 큰 값이 되면서 발산하는 경우도 있다. 이를 기울기 폭주라고 한다. 뒤에서 배울 RNN에서 발생할 수 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ReLU와 ReLU의 변형들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시그모이드 함수를 사용하면 입력의 절댓값이 클 경우에 시그모이드 함수의 출력값이 0 또는 1에 수렴하면서 기울기가 0에 가까워진다. 그래서 역전파 과정에서 전파시킬 기울기가 점차 사라져서 입력층 방향으로 갈수록 역전파가 제대로 되지 않는 기울기 소실 문제가 발생할 수 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울기 소실을 완화하는 가장 간단한 방법은 은닉층의 활성화 함수로 ReLU나 ReLU의 변형 함수를 사용하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 가중치 초기화(Weight initialization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "같은 모델을 훈련시키더라도 가중치가 초기에 어떤 값을 가졌느냐에 따라서 모델의 훈련 결과가 달라지기도 한다. 다시 말해 가중치 초기화만 적절히 해줘도 기울기 소실 문제를 완화시킬 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 세이비어 초기화(Xavier Initialization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "균등 분포로 초기화하는 방법 혹은 정규 분포로 초기화하는 방법, 두 가지 경우로 나뉜다.  \n",
    "이전 층의 뉴런 개수와 다음 층의 뉴런 개수를 가지고 식을 세운다.\n",
    "\n",
    "세이비어 초기화는 여러 층의 기울기 분산 사이에 균형을 맞춰서 특정 층이 너무 주목을 받거나 다른 층이 뒤쳐지는 것을 막는다.  \n",
    "세이비어 초기화는 ReLU와 함께 사용할 경우에는 성능이 좋지 않다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. He 초기화(He initialization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He 초기화 역시 정규 분포와 균등 분포 두 가지 경우로 나뉜다.  \n",
    "다만 He 초기화는 세이비어 초기화와 다르게 다음 층의 뉴런 수를 반영하지 않는다.  \n",
    "\n",
    "- 시그모이드 함수나 하이퍼볼릭탄젠트 함수를 사용할 경우에는 세이비어 초기화 방법이 효율적이다.\n",
    "- ReLU 계열 함수를 사용할 경우에는 He 초기화 방법이 효율적이다.\n",
    "- ReLU + He 초기화 방법이 좀 더 보편적이다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 배치 정규화(Batch Normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 ReLU 계열 함수나, He 초기화를 사용하더라도 훈련 중에 언제든 기울기 소실이나 폭주가 발생할 수 있다. 이를 방지하는 또다른 방법이 바로 배치 정규화이다. 배치 정규화는 인공 신경망의 각 층에 들어가는 입력을 평균과 분산으로 정규화하여 학습을 효율적으로 만든다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 내부 공변량 변화(Internal Covariate Shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "내부 공변량 변화란 학습 과정에서 층 별로 입력 데이터 분포가 달라지는 현상을 말한다.  \n",
    "이전 층들의 학습에 의해 이전 층의 가중치 값이 바뀌게 되면, 현재 층에 전달되는 입력 데이터의 분포가 (현재 층이) 학습했던 시점의 분포와 차이가 발생한다.  \n",
    "배치 정규화를 제안한 논문에서는 기울기 소실/폭주 등의 딥러닝 모델의 불안정성이 층마다 입력의 분포가 달라지기 때문이라고 주장한다.  \n",
    "\n",
    "- 공변량 변화는 훈련 데이터의 분포와 테스트 데이터의 분포가 다른 경우를 의미한다.\n",
    "- 내부 공변량 변화는 **신경망 층 사이에서 발생하는 입력 데이터의 분포 변화**를 의미한다.  \n",
    "- 내부 공변량 변화가 딥러닝 모델의 불안정성의 원인 중 하나라고 논문은 주장한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 배치 정규화(Batch Normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "표현 그대로 한 번에 들어오는 배치 단위로 정규화를 적용하는 것을 말한다.  \n",
    "배치 정규화는 각 층에서 활성화 함수를 통과하기 전에 수행된다.  \n",
    "\n",
    "작동 방식을 요약하면 다음과 같다.  \n",
    "1. 입력에 대해 평균을 0으로 만들고, 정규화를 한다.  \n",
    "2. 정규화 된 데이터에 대해서 스케일과 시프트를 수행한다. 이때 두 개의 매개변수 감마 γ와 베타 β를 사용한다.  \n",
    "감마 γ는 스케일을 위해 사용하고,  \n",
    "베타 β는 시프트를 할 때 사용되며 다음 레이어에 일정한 범위의 값들만 전달되게 한다.\n",
    "\n",
    "배치 정규화의 수식은 링크를 참고하자.  \n",
    "https://wikidocs.net/61271"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- m은 미니 배치에 있는 샘플의 수\n",
    "- μ는 미니 배치 B의 평균\n",
    "- σ는 미니 배치 B의 표준편차\n",
    "\n",
    "- x(i)는 평균이 0이고 정규화된 입력 데이터.\n",
    "\n",
    "- ε은 분모가 0이 되는 것을 막는 작은 수.\n",
    "- γ는 정규화 된 데이터에 대한 스케일 매개변수로 학습 대상\n",
    "- β는 정규화 된 데이터에 대한 시프트 매개변수로 학습 대상\n",
    "- y(i)는 스케일과 시프트를 통해 조정한 BN의 최종 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 정규화는 학습 시 배치 단위의 평균과 분산을 차례대로 받아  \n",
    "이동 평균과 이동 분산을 저장해놓았다가 테스트할 때는 해당 배치의 평균과 분산을 구하지 않고  \n",
    "구해놓았던 평균과 분산으로 정규화한다.  \n",
    "\n",
    "- 배치 정규화를 사용하면 시그모이드 함수, 하이퍼볼릭탄젠트함수를 사용하더라도 기울기 소실 문제가 크게 개선된다.\n",
    "- 가중치 초기화에 훨씬 덜 민감해진다.\n",
    "- 훨씬 큰 학습률을 사용할 수 있어 학습 속도를 개선시킨다.\n",
    "- 미니배치마다 평균과 표준편차를 계산하므로, 훈련 데이터에 일종의 잡음을 넣는 부수 효과를 가져온다. 따라서 과적합을 방지하는 효과가 있다. 부수적이므로, Dropout과 함께 사용하는 것이 좋다.\n",
    "- 배치정규화는 모델을 복잡하게 하며, 추가 계산을 하는 것이므로 테스트 데이터에 대한 예측 시에 실행 시간이 느려진다. 서비스 속도를 고려할 때 배치 정규화가 꼭 필요한지는 고민해봐야한다.  \n",
    "  \n",
    "- 배치 정규화가 꼭 내부 공변량의 변화때문만은 아니라는 논문도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 배치 정규화의 한계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 첫째, 미니 배치 크기에 의존적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 정규화는 너무 작은 배치 크기에서는 잘 동작하지 않을 수 있다.  \n",
    "단적으로 배치 크기를 1로 하게되면 분산은 0이 된다.  \n",
    "작은 미니 배치는 배치 정규화의 효과가 극단적으로 작용되어 훈련에 악영향을 줄 수 있다.  \n",
    "배치 정규화는 크기가 어느정도 되는 미니 배치를 사용하는 것이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 둘째, RNN에 적용하기 어렵다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN은 각 시점마다 다른 통계치를 갖는다.  \n",
    "이는 RNN에 배치 정규화를 적용하는 것을 어렵게 만든다.  \n",
    "대신 layer normalization이라는 방법이 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 층 정규화(Layer Normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상세 내용은 링크의 그림 참조.\n",
    "https://wikidocs.net/61271"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
