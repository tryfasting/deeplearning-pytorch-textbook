{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch로 인공 신경망을 이용한 텍스트 분류를 실습해보자.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 훈련 데이터에 대한 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 텍스트 분류 작업은, Supervised Learning에 속한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 훈련 데이터와 테스트 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 단어에 대한 인덱스 부여"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch의 nn.Embedding()은 단어 각각에 대해 정수가 매핑된 입력에 대해서 임베딩 작업을 수행하게 해준다.  \n",
    "\n",
    "등장 빈도수 순대로 정렬하고, 순차적으로 인덱스를 부여하는 방법이 있다.  \n",
    "장점은 등장 빈도수가 적은 단어를 제거할 수 있다는 것이다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. RNN으로 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m nn.RNN(\u001b[43minput_size\u001b[49m, hidden_size, batch_first=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'input_size' is not defined"
     ]
    }
   ],
   "source": [
    "# 실제 RNN 은닉층을 추가하는 코드. \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "nn.RNN(input_size, hidden_size, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 분류 관점에서 앞서 배운 RNN 코드의 timesteps와 input_dim, 그리고 hidden_size를 해석해보면 다음과 같다.  \n",
    "hidden_size = 출력의 크기 (output_dim)  \n",
    "timesteps = 시점의 수, 각 문서에서의 단어 수.  \n",
    "input_size = 입력의 크기 = 각 단어의 벡터 표현의 차원 수."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. RNN의 Many-to-One 문제\n",
    "\n",
    "텍스트 분류는 RNN의 Many-to-One 문제에 속한다.  \n",
    "즉, 텍스트 분류는 모든 시점(time step)에 대해서 입력을 받지만 최종 시점의 RNN 셀만이 은닉 상태를 출력하고, 이것이 출력층으로 가서 활성화 함수를 통해 정답을 고르는 문제가 된다.  \n",
    "\n",
    "이 때 두 개의 선택지 중에서 정답을 고르는 문제를 Binary Classification 문제,  \n",
    "세 개 이상의 선택지 중에서 정답을 고르는 문제를 Multi-Class Classification 문제라고 한다.  \n",
    "각각의 경우 문제에 맞는 다른 활성화 함수와 손실 함수를 사용한다.  \n",
    "\n",
    "이진 분류 문제의 경우 출력층의 활성화 함수로 시그모이드 함수,  \n",
    "다중 클래스 분류 문제는 출력층 활성화 함수로 소프트맥스 함수를 사용한다.  \n",
    "\n",
    "다중 클래스 분류 문제는 클래스가 N개라면 출력층에 해당되는 밀집층 크기는 N으로 한다.  \n",
    "즉, 출력층의 뉴런 개수는 N개이다.  \n",
    "(단 소프트맥스 함수로 이진 분류를 할 수도 있다. 출력층에 뉴런을 2개로 배치하면 된다.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
