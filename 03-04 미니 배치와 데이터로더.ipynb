{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터를 로드하는 방법과 미니 배치 경사하강법(Minibatch Gradient Descent)에 대해서 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 미니 배치와 배치 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같은 데이터는 모델 학습에는 아쉬운 양이다.   \n",
    "데이터가 수십만개는 필요하다. 그렇다면 빠르게 많은 계산량을 필요로 한다.  \n",
    "그렇기 때문에 전체 데이터를 더 작은 단위로 나누어서 학습하는 개념이 필요하다.  \n",
    "이 단위를 미니 배치(Mini Batch)라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에포크는 전체 훈련 데이터가 학습에 한 번 사용된 주기를 말한다.  \n",
    "위의 경우, 미니 배치의 개수만큼 경사 하강법을 수행해야 1 에포크가 된다.  \n",
    "미니 배치의 크기에 따라 미니 배치의 갯수도 달라지겠다.  \n",
    "미니 배치의 크기를, batch size라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 데이터에 대해서 한 번에 경사 하강법을 수행하는 방법을 '배치 경사하강법',    \n",
    "미니 배치 단위로 경사 하강법을 수행하는 방법을 '미니 배치 경사 하강법'이라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 경사하강법은, 전체 데이터를 사용하므로 가중치값이 최적값에 수렴하는 과정이 매우 안정적이나, 계산량이 너무나 많다.  \n",
    "미니 배치 경사하강법은 전체 데이터의 일부만을 보고 수행하므로, 최적값에 수렴하는 과정이 불안할 수는 있으나, 속도가 빠르다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 크기는 보통 2의 제곱수를 사용한다.  \n",
    "CPU와 GPU의 메모리가 2의 배수이므로 배치크기가 2의 제곱수일 때 데이터 송수신의 효율을 높일 수 있기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 이터레이션(Iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이터레이션이란, 가중치 W와 편향 b가 업데이트되는 횟수를 말한다.   \n",
    "Total data가 2000, Batch size가 200이라고 해보자.  \n",
    "미니 배치 경사하강법에서, 미니 배치의 총 갯수는 10개가 될 것이고,  \n",
    "이터레이션 또한 10이 될 것이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 데이터 로드하기(Data Load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이토치는 데이터를 더 쉽게 다룰 수 있도록 데이터셋(Dataset), 데이터로더(DataLoader)를 제공한다.   \n",
    "이를 사용하면 미니 배치학습, 데이터 셔플(shuffle), 병렬 처리까지 간단히 수행할 수 있다.  \n",
    "기본적인 사용방법은 Dataset을 정의하고, 이를 DataLoader에 전달하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset을 커스텀할 수 있으나, 여기서는 TensorDataset을 사용해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  90], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])  \n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터로더의 인자는 기본적으로 2개이다.  \n",
    "하나는 데이터셋, 다른 하나는 미니배치의 크기이다.  \n",
    "상술한 바, 미니배치 크기는 2의 배수를 보통 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추가적으로 shuffle 인자도 넣는다.  \n",
    "shuffle = True면, Epoch마다 데이터셋을 섞어서 데이터가 학습되는 순서를 바꾼다.  \n",
    "모델이 문제 순서 자체에 익숙해지는 것을 방지할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모델과 옵티마이저를 설계한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(3,1)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Batch 1/3 Cost: 35760.527344\n",
      "Epoch    0/20 Batch 2/3 Cost: 6350.637695\n",
      "Epoch    0/20 Batch 3/3 Cost: 1370.023193\n",
      "Epoch    1/20 Batch 1/3 Cost: 1317.747070\n",
      "Epoch    1/20 Batch 2/3 Cost: 179.465134\n",
      "Epoch    1/20 Batch 3/3 Cost: 128.683441\n",
      "Epoch    2/20 Batch 1/3 Cost: 27.523636\n",
      "Epoch    2/20 Batch 2/3 Cost: 6.914958\n",
      "Epoch    2/20 Batch 3/3 Cost: 6.549057\n",
      "Epoch    3/20 Batch 1/3 Cost: 1.087140\n",
      "Epoch    3/20 Batch 2/3 Cost: 0.442824\n",
      "Epoch    3/20 Batch 3/3 Cost: 0.237659\n",
      "Epoch    4/20 Batch 1/3 Cost: 0.909551\n",
      "Epoch    4/20 Batch 2/3 Cost: 0.548521\n",
      "Epoch    4/20 Batch 3/3 Cost: 0.084948\n",
      "Epoch    5/20 Batch 1/3 Cost: 0.351038\n",
      "Epoch    5/20 Batch 2/3 Cost: 0.745874\n",
      "Epoch    5/20 Batch 3/3 Cost: 0.105970\n",
      "Epoch    6/20 Batch 1/3 Cost: 0.643967\n",
      "Epoch    6/20 Batch 2/3 Cost: 0.289566\n",
      "Epoch    6/20 Batch 3/3 Cost: 0.001647\n",
      "Epoch    7/20 Batch 1/3 Cost: 0.195470\n",
      "Epoch    7/20 Batch 2/3 Cost: 0.563983\n",
      "Epoch    7/20 Batch 3/3 Cost: 0.805244\n",
      "Epoch    8/20 Batch 1/3 Cost: 0.592169\n",
      "Epoch    8/20 Batch 2/3 Cost: 0.315279\n",
      "Epoch    8/20 Batch 3/3 Cost: 0.509672\n",
      "Epoch    9/20 Batch 1/3 Cost: 0.695710\n",
      "Epoch    9/20 Batch 2/3 Cost: 0.231233\n",
      "Epoch    9/20 Batch 3/3 Cost: 0.490380\n",
      "Epoch   10/20 Batch 1/3 Cost: 0.342975\n",
      "Epoch   10/20 Batch 2/3 Cost: 0.606772\n",
      "Epoch   10/20 Batch 3/3 Cost: 0.422314\n",
      "Epoch   11/20 Batch 1/3 Cost: 0.769054\n",
      "Epoch   11/20 Batch 2/3 Cost: 0.350573\n",
      "Epoch   11/20 Batch 3/3 Cost: 0.293557\n",
      "Epoch   12/20 Batch 1/3 Cost: 0.161600\n",
      "Epoch   12/20 Batch 2/3 Cost: 0.946802\n",
      "Epoch   12/20 Batch 3/3 Cost: 0.168241\n",
      "Epoch   13/20 Batch 1/3 Cost: 0.358276\n",
      "Epoch   13/20 Batch 2/3 Cost: 0.218364\n",
      "Epoch   13/20 Batch 3/3 Cost: 0.869519\n",
      "Epoch   14/20 Batch 1/3 Cost: 0.432659\n",
      "Epoch   14/20 Batch 2/3 Cost: 0.400394\n",
      "Epoch   14/20 Batch 3/3 Cost: 0.902871\n",
      "Epoch   15/20 Batch 1/3 Cost: 0.140126\n",
      "Epoch   15/20 Batch 2/3 Cost: 0.311098\n",
      "Epoch   15/20 Batch 3/3 Cost: 0.983286\n",
      "Epoch   16/20 Batch 1/3 Cost: 0.646056\n",
      "Epoch   16/20 Batch 2/3 Cost: 0.328744\n",
      "Epoch   16/20 Batch 3/3 Cost: 0.000286\n",
      "Epoch   17/20 Batch 1/3 Cost: 0.299315\n",
      "Epoch   17/20 Batch 2/3 Cost: 0.213739\n",
      "Epoch   17/20 Batch 3/3 Cost: 0.903741\n",
      "Epoch   18/20 Batch 1/3 Cost: 0.219582\n",
      "Epoch   18/20 Batch 2/3 Cost: 0.513373\n",
      "Epoch   18/20 Batch 3/3 Cost: 0.549959\n",
      "Epoch   19/20 Batch 1/3 Cost: 0.314601\n",
      "Epoch   19/20 Batch 2/3 Cost: 0.115684\n",
      "Epoch   19/20 Batch 3/3 Cost: 1.093935\n",
      "Epoch   20/20 Batch 1/3 Cost: 0.415331\n",
      "Epoch   20/20 Batch 2/3 Cost: 0.546479\n",
      "Epoch   20/20 Batch 3/3 Cost: 0.079299\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        x_train, y_train = samples\n",
    "\n",
    "        # H(x) 계산\n",
    "        prediction = model(x_train)\n",
    "\n",
    "        # cost 계산\n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "        # cost로 H(x) 계산\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, batch_idx+1, len(dataloader),\n",
    "        cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 커스텀 데이터셋 (Custom Dataset) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.utils.dats.Dataset을 상속받아 직접 커스텀 데이터셋을 만드는 경우도 있다.  \n",
    "torch.utils.data.Dataset은 파이토치에서 데이터셋을 제공하는 추상 클래스이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본적인 뼈대\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        # 데이터셋의 전처리를 해주는 부분\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 커스텀 데이터셋으로 선형 회귀 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 상속\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x_data = [[73, 80, 75],\n",
    "                   [93, 88, 93],\n",
    "                   [89, 91, 90],\n",
    "                   [96, 98, 100],\n",
    "                   [73, 66, 70]]\n",
    "        self.y_data = [[152], [185], [180], [196], [142]] \n",
    "\n",
    "    # 총 데이터의 개수를 리턴\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.FloatTensor(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(3,1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Batch 1/3 Cost: 85282.453125\n",
      "Epoch    0/20 Batch 2/3 Cost: 14186.439453\n",
      "Epoch    0/20 Batch 3/3 Cost: 11486.381836\n",
      "Epoch    1/20 Batch 1/3 Cost: 1625.456909\n",
      "Epoch    1/20 Batch 2/3 Cost: 566.051514\n",
      "Epoch    1/20 Batch 3/3 Cost: 147.327728\n",
      "Epoch    2/20 Batch 1/3 Cost: 55.996269\n",
      "Epoch    2/20 Batch 2/3 Cost: 22.234821\n",
      "Epoch    2/20 Batch 3/3 Cost: 1.620827\n",
      "Epoch    3/20 Batch 1/3 Cost: 0.151190\n",
      "Epoch    3/20 Batch 2/3 Cost: 8.265091\n",
      "Epoch    3/20 Batch 3/3 Cost: 7.487826\n",
      "Epoch    4/20 Batch 1/3 Cost: 2.591959\n",
      "Epoch    4/20 Batch 2/3 Cost: 5.115668\n",
      "Epoch    4/20 Batch 3/3 Cost: 3.430770\n",
      "Epoch    5/20 Batch 1/3 Cost: 5.220237\n",
      "Epoch    5/20 Batch 2/3 Cost: 3.468729\n",
      "Epoch    5/20 Batch 3/3 Cost: 1.697594\n",
      "Epoch    6/20 Batch 1/3 Cost: 3.449394\n",
      "Epoch    6/20 Batch 2/3 Cost: 1.917554\n",
      "Epoch    6/20 Batch 3/3 Cost: 10.997640\n",
      "Epoch    7/20 Batch 1/3 Cost: 2.558268\n",
      "Epoch    7/20 Batch 2/3 Cost: 5.090144\n",
      "Epoch    7/20 Batch 3/3 Cost: 3.333734\n",
      "Epoch    8/20 Batch 1/3 Cost: 7.939080\n",
      "Epoch    8/20 Batch 2/3 Cost: 4.813897\n",
      "Epoch    8/20 Batch 3/3 Cost: 0.826830\n",
      "Epoch    9/20 Batch 1/3 Cost: 0.686736\n",
      "Epoch    9/20 Batch 2/3 Cost: 4.755189\n",
      "Epoch    9/20 Batch 3/3 Cost: 9.257616\n",
      "Epoch   10/20 Batch 1/3 Cost: 5.078943\n",
      "Epoch   10/20 Batch 2/3 Cost: 6.921024\n",
      "Epoch   10/20 Batch 3/3 Cost: 4.223371\n",
      "Epoch   11/20 Batch 1/3 Cost: 1.187778\n",
      "Epoch   11/20 Batch 2/3 Cost: 9.087991\n",
      "Epoch   11/20 Batch 3/3 Cost: 4.748044\n",
      "Epoch   12/20 Batch 1/3 Cost: 3.635482\n",
      "Epoch   12/20 Batch 2/3 Cost: 1.451103\n",
      "Epoch   12/20 Batch 3/3 Cost: 10.654941\n",
      "Epoch   13/20 Batch 1/3 Cost: 3.415406\n",
      "Epoch   13/20 Batch 2/3 Cost: 5.565429\n",
      "Epoch   13/20 Batch 3/3 Cost: 4.183391\n",
      "Epoch   14/20 Batch 1/3 Cost: 4.353499\n",
      "Epoch   14/20 Batch 2/3 Cost: 2.989940\n",
      "Epoch   14/20 Batch 3/3 Cost: 4.473054\n",
      "Epoch   15/20 Batch 1/3 Cost: 3.662368\n",
      "Epoch   15/20 Batch 2/3 Cost: 4.664465\n",
      "Epoch   15/20 Batch 3/3 Cost: 1.810465\n",
      "Epoch   16/20 Batch 1/3 Cost: 8.046975\n",
      "Epoch   16/20 Batch 2/3 Cost: 4.358182\n",
      "Epoch   16/20 Batch 3/3 Cost: 1.063382\n",
      "Epoch   17/20 Batch 1/3 Cost: 5.708055\n",
      "Epoch   17/20 Batch 2/3 Cost: 3.532766\n",
      "Epoch   17/20 Batch 3/3 Cost: 1.636366\n",
      "Epoch   18/20 Batch 1/3 Cost: 5.509888\n",
      "Epoch   18/20 Batch 2/3 Cost: 1.616306\n",
      "Epoch   18/20 Batch 3/3 Cost: 7.459043\n",
      "Epoch   19/20 Batch 1/3 Cost: 4.225540\n",
      "Epoch   19/20 Batch 2/3 Cost: 2.777884\n",
      "Epoch   19/20 Batch 3/3 Cost: 4.084672\n",
      "Epoch   20/20 Batch 1/3 Cost: 3.145100\n",
      "Epoch   20/20 Batch 2/3 Cost: 5.170113\n",
      "Epoch   20/20 Batch 3/3 Cost: 2.122718\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        # print(batch_idx)\n",
    "        # print(samples)\n",
    "        x_train, y_train = samples\n",
    "\n",
    "        # H(x) 계산\n",
    "        prediction = model(x_train)\n",
    "\n",
    "        # cost 계산\n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "        # cost로 H(x) 계산\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, batch_idx+1, len(dataloader),\n",
    "        cost.item()\n",
    "        ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
