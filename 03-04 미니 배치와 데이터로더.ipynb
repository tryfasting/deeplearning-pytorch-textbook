{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터를 로드하는 방법과 미니 배치 경사하강법(Minibatch Gradient Descent)에 대해서 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 미니 배치와 배치 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같은 데이터는 모델 학습에는 아쉬운 양이다.   \n",
    "데이터가 수십만개는 필요하다. 그렇다면 빠르게 많은 계산량을 필요로 한다.  \n",
    "그렇기 때문에 전체 데이터를 더 작은 단위로 나누어서 학습하는 개념이 필요하다.  \n",
    "이 단위를 미니 배치(Mini Batch)라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에포크는 전체 훈련 데이터가 학습에 한 번 사용된 주기를 말한다.  \n",
    "위의 경우, 미니 배치의 개수만큼 경사 하강법을 수행해야 1 에포크가 된다.  \n",
    "미니 배치의 크기에 따라 미니 배치의 갯수도 달라지겠다.  \n",
    "미니 배치의 크기를, batch size라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 데이터에 대해서 한 번에 경사 하강법을 수행하는 방법을 '배치 경사하강법',    \n",
    "미니 배치 단위로 경사 하강법을 수행하는 방법을 '미니 배치 경사 하강법'이라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 경사하강법은, 전체 데이터를 사용하므로 가중치값이 최적값에 수렴하는 과정이 매우 안정적이나, 계산량이 너무나 많다.  \n",
    "미니 배치 경사하강법은 전체 데이터의 일부만을 보고 수행하므로, 최적값에 수렴하는 과정이 불안할 수는 있으나, 속도가 빠르다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 크기는 보통 2의 제곱수를 사용한다.  \n",
    "CPU와 GPU의 메모리가 2의 배수이므로 배치크기가 2의 제곱수일 때 데이터 송수신의 효율을 높일 수 있기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 이터레이션(Iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이터레이션이란, 가중치 W와 편향 b가 업데이트되는 횟수를 말한다.   \n",
    "Total data가 2000, Batch size가 200이라고 해보자.  \n",
    "미니 배치 경사하강법에서, 미니 배치의 총 갯수는 10개가 될 것이고,  \n",
    "이터레이션 또한 10이 될 것이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 데이터 로드하기(Data Load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이토치는 데이터를 더 쉽게 다룰 수 있도록 데이터셋(Dataset), 데이터로더(DataLoader)를 제공한다.   \n",
    "이를 사용하면 미니 배치학습, 데이터 셔플(shuffle), 병렬 처리까지 간단히 수행할 수 있다.  \n",
    "기본적인 사용방법은 Dataset을 정의하고, 이를 DataLoader에 전달하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset을 커스텀할 수 있으나, 여기서는 TensorDataset을 사용해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  90], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])  \n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터로더의 인자는 기본적으로 2개이다.  \n",
    "하나는 데이터셋, 다른 하나는 미니배치의 크기이다.  \n",
    "상술한 바, 미니배치 크기는 2의 배수를 보통 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추가적으로 shuffle 인자도 넣는다.  \n",
    "shuffle = True면, Epoch마다 데이터셋을 섞어서 데이터가 학습되는 순서를 바꾼다.  \n",
    "모델이 문제 순서 자체에 익숙해지는 것을 방지할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모델과 옵티마이저를 설계한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(3,1)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=1e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
