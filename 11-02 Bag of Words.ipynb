{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tryfasting/deeplearning-pytorch-textbook/blob/main/11-02%20Bag%20of%20Words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbauDLyJB8Lk"
      },
      "source": [
        "## 단어의 등장 순서를 고려하지 않는 빈도수 기반 단어 표현 방법인 Bag of Words에 대해서 배운다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbiLZ13AB8Ll"
      },
      "source": [
        "### 1. Bag of Words란"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVLyIIPDB8Ll"
      },
      "source": [
        "단어들의 순서는 고려하지 않고, 단어들의 frequency에만 집중한다.    \n",
        "\n",
        "1. 각 단어에 고유한 정수 인덱스를 부여한다.  \n",
        "2. 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만든다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clGCrIeWB8Lm"
      },
      "source": [
        "**문서 1 : 정부가 발표하는 물가상승률과 소비자가 느기는 물가상승률은 다르다.**  \n",
        "\n",
        "에 대해서 BoW를 만들어보자."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "RtCOotKICOJy",
        "outputId": "2ed56d5a-2faf-41b8-e420-fb90b4e9b6a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.3.1)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.11/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1>=0.7.0->konlpy) (24.2)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.1/494.1 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.2 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Wim0A47CB8Lm"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Okt\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "def build_bag_of_words(document):\n",
        "    # 온점 제거 및 형태소 분석\n",
        "    document = document.replace('.','')\n",
        "    tokenized_document = okt.morphs(document)\n",
        "\n",
        "    word_to_index = {}\n",
        "    bow = []\n",
        "\n",
        "    for word in tokenized_document:\n",
        "        if word not in word_to_index.keys():\n",
        "            # 자동으로 새로운 단어에 value를 len(word_to_index)만큼 부여\n",
        "            # 새로운 value는 유일한 값을 갖는 것을 보장받는다.\n",
        "            # 따라서 인덱스는 중복되지 않고 부여된다.\n",
        "            word_to_index[word] = len(word_to_index)\n",
        "            # Bow에 전부 기본값 1를 넣는다.ㅠ\n",
        "            bow.insert(len(word_to_index)-1, 1)\n",
        "        else:\n",
        "            # 재등장하는 단어의 인덱스\n",
        "            index = word_to_index.get(word)\n",
        "            # 재등장한 단어는 해당하는 인덱스의 위치에 1을 더한다\n",
        "            bow[index] = bow[index] + 1\n",
        "\n",
        "            # word_to_index 말 그대로, 단어에 인덱스를 부여해 딕셔너리로 만든다.\n",
        "            # 즉 Bow의 해당 인덱스에 접근하면, 단어의 frequency를 구할 수 있다.\n",
        "\n",
        "    return word_to_index, bow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "해당 함수에 문서1을 입력으로 넣어보자."
      ],
      "metadata": {
        "id": "V121XB96OYdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = '정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.'\n",
        "vocab, bow = build_bag_of_words(doc1)\n",
        "print('vocabulary : ', vocab)\n",
        "print('bag of words vector : ',bow)"
      ],
      "metadata": {
        "id": "ke5m7zU3Oedz",
        "outputId": "f525ed81-5607-4ee3-84cc-8535b8e498b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary :  {'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\n",
            "bag of words vector :  [1, 2, 1, 1, 2, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "만약 한국어에서 불용어에 해당하는 조사들을 제거한다면 더 정제된 BoW를 만들 수도 있겠다."
      ],
      "metadata": {
        "id": "Z9tO6kIhPx6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Bag of Words의 다른 예제들"
      ],
      "metadata": {
        "id": "awCvDszdP8KB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**문서 2 : 소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.**  \n",
        "\n",
        "위의 함수에 임의의 문서 2를 입력하여 결과를 확인해보자."
      ],
      "metadata": {
        "id": "CN-IhzomQCYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc2 = '소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.'\n",
        "vocab, bow = build_bag_of_words(doc2)\n",
        "print('vocabulary : ', vocab)\n",
        "print('bag of words vector : ',bow)"
      ],
      "metadata": {
        "id": "rrjm_n9oQPFF",
        "outputId": "bc772943-23aa-435b-e36b-0b5bcabb3652",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary :  {'소비자': 0, '는': 1, '주로': 2, '소비': 3, '하는': 4, '상품': 5, '을': 6, '기준': 7, '으로': 8, '물가상승률': 9, '느낀다': 10}\n",
            "bag of words vector :  [1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문서 1, 문서 2를 합쳐서 문서 3이라고 명명하고, BoW를 만들 수도 있다.  "
      ],
      "metadata": {
        "id": "LbQelQ6cQZl5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**문서3 : 정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다. 소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.**"
      ],
      "metadata": {
        "id": "nltuTB2fQhhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc3 = doc1 + ' ' + doc2\n",
        "vocab, bow = build_bag_of_words(doc3)\n",
        "print('vocabulary :', vocab)\n",
        "print('bag of words vector :', bow)"
      ],
      "metadata": {
        "id": "GWWZXeRCQ0OT",
        "outputId": "f01f03a8-f7a1-406b-fb9f-22882b06edd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary : {'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9, '는': 10, '주로': 11, '소비': 12, '상품': 13, '을': 14, '기준': 15, '으로': 16, '느낀다': 17}\n",
            "bag of words vector : [1, 2, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BoW는 종종 여러 문서의 단어 집합을 합친 뒤에, 해당 단어 집합에 대한 각 문서의 BoW를 구하기도 한다.  \n",
        "가령, 문서3에 대한 단어 집합을 기준으로 문서1, 문서2의 BoW를 만들면 결과는 아래와 같다."
      ],
      "metadata": {
        "id": "Gf1balP7Q8x9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "문서3 단어 집합에 대한 문서1 BoW : [1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
        "문서3 단어 집합에 대한 문서2 BoW : [0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 2, 1, 1, 1]  \n"
      ],
      "metadata": {
        "id": "15ysRHISRokj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BoW는 각 단어가 등장한 횟수를 수치화하는 텍스트 표현 방법이므로 문서가 어떤 성격의 문서인지를 판단하는 작업에 쓰인다."
      ],
      "metadata": {
        "id": "3bLVu5xmRty0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. CountVectorizer 클래스로 BoW 만들기"
      ],
      "metadata": {
        "id": "T5jTIxqBR7OF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "사이킷런의 CountVectorizer 클래스로 BoW를 만들어보자."
      ],
      "metadata": {
        "id": "KkVCloywSBYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = ['you know i want your love. because I love you.']\n",
        "vector = CountVectorizer()\n",
        "\n",
        "# 코퍼스로부터 각 단어의 빈도수를 기록\n",
        "print('bag of words vector : ', vector.fit_transform(corpus).toarray())\n",
        "\n",
        "# 각 단어의 인덱스가 어떻게 부여되었는지를 출력\n",
        "print('vocabulary : ', vector.vocabulary_)"
      ],
      "metadata": {
        "id": "eN_CZpkgSIEW",
        "outputId": "9df97041-30a1-4101-b7e7-8aacde586335",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bag of words vector :  [[1 1 2 1 2 1]]\n",
            "vocabulary :  {'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "기본적으로 CountVectorizer는 길이가 2 이상인 문자에서는 토큰으로 인식하기 때문에, 'I' 같은 문자는 빠졌다. 정제(Cleaning) 챕터에서 언급했듯이, 영어에서는 길이가 짧은 문자를 제거하는 것 또한 전처리 작업으로 고려되기도 한다.  "
      ],
      "metadata": {
        "id": "wrYilnJPS6gt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " CountVectorizer는 띄어쓰기를 기준으로 단어를 자르기 때문에, 한국어에는 적용하는 건 적절치 않다."
      ],
      "metadata": {
        "id": "6fXppyvfTtpZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 불용어를 제거한 BoW 만들기"
      ],
      "metadata": {
        "id": "Ph6LpvhyT3l3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BoW를 만들어서, 각 단어에 대한 빈도수를 수치화하겠다는 것은, 즉 텍스트 내에서 어떤 단어가 중요한지 살펴보겠다는 뜻이다. 그렇다면 불용어를 제거해서 자연어처리의 정확도를 높이는 것이 좋겠다."
      ],
      "metadata": {
        "id": "VruYW6jTT6Xd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "영어는 CountVectorizer에서, 불용어를 제외하고 BoW를 만들어주는 기능을 지원한다."
      ],
      "metadata": {
        "id": "oAFUHyyoUJET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "vtdsSsw7UTlm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1) 사용자가 직접 정의한 불용어 사용"
      ],
      "metadata": {
        "id": "uUDJTjO8UbxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = ['Family is not an important thing. It\\'s everything.']\n",
        "vect = CountVectorizer(stop_words=['the','a','an','is','not'])\n",
        "\n",
        "print('bag of words vector :',vect.fit_transform(text).toarray())\n",
        "print('vocabulary :', vect.vocabulary_)"
      ],
      "metadata": {
        "id": "MCYAIbNmUyAe",
        "outputId": "24219a6d-2210-4c40-c2e8-72b7eb6af82d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bag of words vector : [[1 1 1 1 1]]\n",
            "vocabulary : {'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2) CountVectorizer에서 제공하는 자체 불용어 사용"
      ],
      "metadata": {
        "id": "BmdLRzCiU_Xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = ['Family is not an important thing. It\\'s everything.']\n",
        "vect = CountVectorizer(stop_words='english')\n",
        "\n",
        "print('bag of words vector :',vect.fit_transform(text).toarray())\n",
        "print('vocabulary :', vect.vocabulary_)"
      ],
      "metadata": {
        "id": "xf5THRv-VGo7",
        "outputId": "6deca277-be14-475b-c1a1-a13e99c5331b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bag of words vector : [[1 1 1]]\n",
            "vocabulary : {'family': 0, 'important': 1, 'thing': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3) NLTK에서 지원하는 불용어 사용"
      ],
      "metadata": {
        "id": "lOkE7ZFqVNEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "hP-3qVxwVcfP",
        "outputId": "16d7889c-cf5a-4eb7-d430-7f6d7f262226",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = ['Family is not an important thing. It\\'s everything.']\n",
        "stop_words = stopwords.words('english')\n",
        "vect = CountVectorizer(stop_words=stop_words)\n",
        "print('bag of words vector :',vect.fit_transform(text).toarray())\n",
        "print('vocabulary :', vect.vocabulary_)"
      ],
      "metadata": {
        "id": "so69bLOIVQJZ",
        "outputId": "93cbb0ba-c1ce-4de6-c09f-261f5517479c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bag of words vector : [[1 1 1 1]]\n",
            "vocabulary : {'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}