{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 통계적 언어 모델이 통계적인 접근 방법으로 어떻게 언어를 모델링 하는지 배워보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 조건부 확률\n",
    "\n",
    "조건부 확률의 연쇄 법칙  \n",
    "$P(x_1, x_2, x_3 ... x_n) = P(x_1)P(x_2|x_1)P(x_3|x_1,x_2)...P(x_n|x_1 ... x_{n-1})$  \n",
    "\n",
    "예시\n",
    "$P(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 문장에 대한 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장 'An adorable little boy is spreading smiles'의 확률 $P(\\text{An adorable little boy is spreading smiles})$를 식으로 표현해보자.  \n",
    "\n",
    "각 단어는 문맥이라는 관계로 인해 이전 단어의 영향을 받아 나온 단어이다. 그리고 모든 단어로부터 하나의 문장이 완성된다.  \n",
    "\n",
    "앞서 언급한 조건부 확률의 일반화 식을 문장의 확률 관점에서 다시 적어보면 문장의 확률은 각 단어들이 이전 단어가 주어졌을 때 다음 단어로 등장할 확률의 곱으로 구성된다.  \n",
    "$P(w_1, w_2, w_3, w_4, w_5, ... w_n) = \\prod_{n=1}^{n}P(w_{n} | w_{1}, ... , w_{n-1})$\n",
    "\n",
    "위의 문장에 해당 식을 적용해보면 다음과 같다.\n",
    "\n",
    "$P(\\text{An adorable little boy is spreading smiles}) =$\n",
    "\n",
    "$P(\\text{An})  ×  P(\\text{adorable|An})  ×  P(\\text{little|An adorable})  ×  P(\\text{boy|An adorable little})\n",
    "         ×  P(\\text{is|An adorable little boy})$\n",
    "\n",
    "문장의 확률을 구하기 위해 각 단어에 대한 예측 확률들을 곱한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 카운트 기반의 접근"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SLM은 이전 단어로부터 다음 단어에 대한 확률을 어떻게 구하는가?    \n",
    "정답은 카운트(빈도 수)에 기반하여 확률을 계산한다.  \n",
    "$P\\text{(is|An adorable little boy}) = \\frac{\\text{count(An adorable little boy is})}{\\text{count(An adorable little boy })}$\n",
    "\n",
    "예를 들어 기계가 학습한 corpus 데이터에서 An adorable little boy가 100번 등장했는데, 그 다음 is가 등장한 경우는 30번이라 하자. 그렇다면 $P(\\text{is|An adorable little boy})$는 30%가 되는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 카운트 기반 접근의 한계 - 희소 문제(Sparsity Problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "언어 모델은 실생활에서 사용되는 언어의 확률 분포를 근사 모델링한다. 실제로는 정확하게 알아보기 어렵겠지만 현실에서도 'An adorable little boy'가 나왔을 때 'is'가 나올 확률이 존재할 것이다. 이를 실제 자연어의 확률분포, 현실에서의 확률 분포라고 하자.  \n",
    "기계에게 많은 코퍼스를 훈련시켜서 언어 모델을 통해 현실에서의 확률 분포를 근사하는 것이 언어 모델의 목표이다.  \n",
    "그런데 카운트 기반으로 접근하면, 필요로 하는 코퍼스, 즉 기계가 훈련해야 하는 데이터는 정말 방대한 양이 필요하다.  \n",
    "$P\\text{(is|An adorable little boy}) = \\frac{\\text{count(An adorable little boy is})}{\\text{count(An adorable little boy })}$\n",
    "\n",
    "만약 위와 같이 $P\\text{(is|An adorable little boy})$의 확률을 구하는데, 기계가 훈련한 코퍼스에  \n",
    "'An adorable little boy is'라는 단어 시퀀스가 없었다면, 이 단어 시퀀스에 대한 확률은 그냥 0이 되어버린다.  \n",
    "혹은 'An adorable little boy'라는 단어 시퀀스가 없다면 분모가 0이 되어버려 확률이 정의되지 않는다.  \n",
    "\n",
    "이와 같이 충분한 데이터를 관측하지 못하여 언어를 정확히 모델링 하지 못하는 문제를 **희소 문제**(**sparsity problem**)이라고 한다.\n",
    "\n",
    "위 문제를 완화하는 방법으로  \n",
    "n-gram 언어 모델이나,  \n",
    "스무딩, 백오프와 같은 여러가지 일반화(generalization) 기법이 존재한다.  \n",
    "\n",
    "그러나 근본적인 해결책은 못 된다.  \n",
    "이후 인공 신경망 언어 모델로 트렌드가 넘어가게 된다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. N-gram 언어 모델\n",
    "\n",
    "N-gram 역시 카운트에 기반한 통계적 접근이므로, SLM의 일종이다. 다만 등장한 모든 단어를 고려하는 것이 아닌, 일부 단어만 고려하는 접근 방법을 사용한다.  \n",
    "이때 일부 단어를 몇 개 보느냐를 결정하는 것이 'N-gram'의 'N'이 되겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) N-gram 언어 모델의 접근 방법\n",
    "\n",
    "SLM의 한계는, 훈련 코퍼스에 확률을 계산하고 싶은 문장이나 단어가 없을 수 있다는 점이었다.  \n",
    "그런데 참고하는 단어의 수를 줄이면 카운트를 할 수 있는 가능성을 높일 수 있다.  \n",
    "\n",
    "즉, 이제는 앞 단어 중 임의의 개수만 포함해서 카운트하여 근사하여 접근한다. 이렇게 하면 갖고 있는 코퍼스에서 해당 단어의 시퀀스를 카운트할 확률이 높아질 것이기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) N-gram\n",
    "이때, 임의의 개수를 정하기 위한 기준이 바로 n-gram이다.  \n",
    "n-gram은 n개의 연속적인 단어 나열을 의미한다.\n",
    "\n",
    "예컨대 다음과 같다.\n",
    "`An adorable little boy is spreading smiles`\n",
    "\n",
    "**unigrams** : an, adorable, little, boy, is, spreading, smiles  \n",
    "**bigrams** : an adorable, adorable little, little boy, boy is, is spreading, spreading smiles  \n",
    "**trigrams** : an adorable little, adorable little boy, little boy is, boy is spreading, is spreading smiles  \n",
    "**4-grams** : an adorable little boy, adorable little boy is, little boy is spreading, boy is spreading smiles  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-gram을 통한 언어 모델에서는 다음에 나올 단어의 예측은 오직 n-1개의 단어에만 의존한다.  \n",
    "  \n",
    "예를 들어 'An adorable little boy is spreading' 다음에 나올 단어를 예측하고 싶다고 할 때, n=4라고 한 4-gram을 이용한 언어 모델을 사용한다고 하자.  \n",
    "이 경우, 'spreading' 다음에 올 단어를 예측하는 것은 n-1에 해당되는 앞의 3개의 단어만을 고려한다.  \n",
    "즉 An adorable little은 무시한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(\"./scr/10-02 n-gram.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(w\\text{|boy is spreading}) = \\frac{\\text{count(boy is spreading}\\ w)}{\\text{count(boy is spreading)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) N-gram Language Model의 한계\n",
    "\n",
    "전체 문장을 고려한 언어 모델보다는 정확도가 떨어질 수밖에 없다.\n",
    "\n",
    "- **희소 문제(Sparsity Problem)**  \n",
    "    여전히 희소 문제가 존재한다.\n",
    "- **n을 선택하는 것은 trade-off 문제.**\n",
    "    n을 크게 선택한다 -> 희소 문제 발생\n",
    "    희소 문제 축소 -> n을 작게 선택해야 한다.  \n",
    "    **n은 최대 5를 넘게 잡아서는 안된다고 권장되고 있다.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
