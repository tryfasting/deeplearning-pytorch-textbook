{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 파이토치에서 이미 구현된 함수들을 불러와서 더 쉽게 선형 회귀 모델을 구현해본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 단순 선형 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x22a2a27ae50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1],[2],[3]])\n",
    "y_train = torch.FloatTensor([[2],[4],[6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dim = 1, output_dim = 1\n",
    "# 하나의 입력 x, 하나의 출력 y이므로, 모두 1을 넣었다.\n",
    "model = nn.Linear(1,1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.1939]], requires_grad=True), Parameter containing:\n",
      "tensor([0.4694], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# model에 저장되어 있는 가중치 W와 편향 b\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer 설정. 경사하강법 SGD를 사용하고 learning rate는 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 14.714844\n",
      "Epoch  100/2000 Cost: 0.127436\n",
      "Epoch  200/2000 Cost: 0.078748\n",
      "Epoch  300/2000 Cost: 0.048661\n",
      "Epoch  400/2000 Cost: 0.030070\n",
      "Epoch  500/2000 Cost: 0.018581\n",
      "Epoch  600/2000 Cost: 0.011482\n",
      "Epoch  700/2000 Cost: 0.007095\n",
      "Epoch  800/2000 Cost: 0.004384\n",
      "Epoch  900/2000 Cost: 0.002709\n",
      "Epoch 1000/2000 Cost: 0.001674\n",
      "Epoch 1100/2000 Cost: 0.001035\n",
      "Epoch 1200/2000 Cost: 0.000639\n",
      "Epoch 1300/2000 Cost: 0.000395\n",
      "Epoch 1400/2000 Cost: 0.000244\n",
      "Epoch 1500/2000 Cost: 0.000151\n",
      "Epoch 1600/2000 Cost: 0.000093\n",
      "Epoch 1700/2000 Cost: 0.000058\n",
      "Epoch 1800/2000 Cost: 0.000036\n",
      "Epoch 1900/2000 Cost: 0.000022\n",
      "Epoch 2000/2000 Cost: 0.000014\n"
     ]
    }
   ],
   "source": [
    "# 전체 훈련 데이터에 대해 경사하강법을 2,000회 반복\n",
    "nb_epochs = 2000\n",
    "for epoch in range(np_epochs+1):\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    # pytorch에서 제공하는 평균제곱오차 함수\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    # 비용함수를 미분하여 gradient 계산\n",
    "    cost.backward()\n",
    "    # W와 b를 업데이트\n",
    "    optimizer.step()    \n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 4일 때의 예측값 : tensor([[7.9926]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 입력 4 선언\n",
    "new_var = torch.FloatTensor([[4.0]])\n",
    "\n",
    "# 입력할 값 4에 대해서 예측값 y^를 리턴받아서 pred_y에 저장\n",
    "pred_y = model(new_var)\n",
    "\n",
    "print(f'훈련 후 입력이 4일 때의 예측값 : {pred_y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W의 값이 2에 가깝고, b의 값이 0에 가까운 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[1.9957]], requires_grad=True), Parameter containing:\n",
      "tensor([0.0097], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- H(x) 식에 입력 x로부터 예측된 y를 얻는 것을 forward 연산이라고 한다.\n",
    "- 학습 전, prediction = model(x_train)은 x_train으로부터 예측값을 리턴하므로 forward연산\n",
    "- 학습 후, pred_y = model(new_var)는 임의의 값 new_var로부터 예측값을 리턴하므로 forward연산\n",
    "\n",
    "- cost.backward()는 비용 함수로부터 기울기를 구하라는 의미이며 backward 연산이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
